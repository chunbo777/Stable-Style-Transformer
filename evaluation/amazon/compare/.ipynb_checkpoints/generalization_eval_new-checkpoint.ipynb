{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "output_path = '/DATA/joosung/controllable_english/evaluation/amazon/compare/amazon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  CrossAligned\n",
      "BLEU score: 0.7588409369895635\n",
      "model:  DeleteAndRetrieve\n",
      "BLEU score: 60.750196051433\n",
      "model:  human\n",
      "BLEU score: 47.66531309271789\n",
      "model:  multi_decoder\n",
      "BLEU score: 16.480285854797188\n",
      "model:  RetrieveOnly\n",
      "BLEU score: 2.821412441454428\n",
      "model:  StyleEmbedding\n",
      "BLEU score: 32.034407655633125\n",
      "model:  TemplateBased\n",
      "BLEU score: 68.53918914192806\n",
      "model:  B_GST\n",
      "BLEU score: 58.21272247148338\n",
      "model:  G_GST\n",
      "BLEU score: 51.024381849754874\n",
      "model:  DeleteOnly\n",
      "BLEU score: 57.476752207745974\n",
      "model:  input_copy\n",
      "BLEU score: 100.0\n"
     ]
    }
   ],
   "source": [
    "### self-BLEU\n",
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_bleu_1 = 0\n",
    "    neg_bleu_2 = 0\n",
    "    neg_bleu_3 = 0\n",
    "    neg_bleu_4 = 0\n",
    "    neg_bleu_score = 0\n",
    "    for k in range(neg_len):\n",
    "        ori_sen = neg_data_dataset[k].split('\\t')[0]\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_1 += bleu_1_score\n",
    "        neg_bleu_2 += bleu_2_score\n",
    "        neg_bleu_3 += bleu_3_score\n",
    "        neg_bleu_4 += bleu_4_score\n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_bleu_1 = 0\n",
    "    pos_bleu_2 = 0\n",
    "    pos_bleu_3 = 0\n",
    "    pos_bleu_4 = 0\n",
    "    pos_bleu_score = 0\n",
    "    for k in range(pos_len):\n",
    "        ori_sen = pos_data_dataset[k].split('\\t')[0]\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_1 += bleu_1_score\n",
    "        pos_bleu_2 += bleu_2_score\n",
    "        pos_bleu_3 += bleu_3_score\n",
    "        pos_bleu_4 += bleu_4_score\n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    bleu_1 = (neg_bleu_1+pos_bleu_1)/(neg_len+pos_len)\n",
    "    bleu_2 = (neg_bleu_2+pos_bleu_2)/(neg_len+pos_len)\n",
    "    bleu_3 = (neg_bleu_3+pos_bleu_3)/(neg_len+pos_len)\n",
    "    bleu_4 = (neg_bleu_4+pos_bleu_4)/(neg_len+pos_len)\n",
    "    blue_score = (neg_bleu_score+pos_bleu_score)/(neg_len+pos_len)\n",
    "#     print('BLEU_1 score: {}'.format(bleu_1))\n",
    "#     print('BLEU_2 score: {}'.format(bleu_2))\n",
    "#     print('BLEU_3 score: {}'.format(bleu_3))\n",
    "#     print('BLEU_4 score: {}'.format(bleu_4))\n",
    "#     print('Average: ', (bleu_1+bleu_2+bleu_3+bleu_4)/4*100)\n",
    "    print('BLEU score: {}'.format(blue_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  CrossAligned\n",
      "BLEU score: 0.6127331590644414\n",
      "model:  DeleteAndRetrieve\n",
      "BLEU score: 30.827953651606617\n",
      "model:  human\n",
      "BLEU score: 97.8\n",
      "model:  multi_decoder\n",
      "BLEU score: 6.605157342832834\n",
      "model:  RetrieveOnly\n",
      "BLEU score: 1.225719442942976\n",
      "model:  StyleEmbedding\n",
      "BLEU score: 12.95277687362883\n",
      "model:  TemplateBased\n",
      "BLEU score: 33.79117417718371\n",
      "model:  B_GST\n",
      "BLEU score: 25.467824606429122\n",
      "model:  G_GST\n",
      "BLEU score: 21.101497574713434\n",
      "model:  DeleteOnly\n",
      "BLEU score: 28.563816703038547\n",
      "model:  input_copy\n",
      "BLEU score: 47.604945833277235\n"
     ]
    }
   ],
   "source": [
    "### human-BLEU\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/amazon/compare1/amazon/'\n",
    "neg_human = human_path + 'sentiment.test.0.human'\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = human_path + 'sentiment.test.1.human'\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_bleu_score = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        \n",
    "        ### human\n",
    "        ori_sen = neg_human_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)        \n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_bleu_score = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "\n",
    "        ### human\n",
    "        ori_sen = pos_human_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)        \n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    blue_score = (neg_bleu_score+pos_bleu_score)/(neg_len+pos_len)\n",
    "    print('BLEU score: {}'.format(blue_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import *\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "import json\n",
    "f = open('../amazon_vocab.json')\n",
    "token2num = json.load(f)\n",
    "\n",
    "num2token = {}\n",
    "for key, value in token2num.items():\n",
    "    num2token[value] = key\n",
    "import sys    \n",
    "sys.path.insert(0, \"/DATA/joosung/controllable_english/amazon/visual_v1_1/\")\n",
    "from dis_model import *\n",
    "dismodel = findattribute().cuda()\n",
    "\n",
    "i='5'\n",
    "dismodel_name='cls_model_' + str(i)\n",
    "dismodel.load_state_dict(torch.load('/DATA/joosung/controllable_english/amazon/visual_v1_1/models/{}'.format(dismodel_name)))\n",
    "dismodel.eval()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  CrossAligned\n",
      "Accuracy: 74.8%\n",
      "model:  DeleteAndRetrieve\n",
      "Accuracy: 52.400000000000006%\n",
      "model:  human\n",
      "Accuracy: 46.9%\n",
      "model:  multi_decoder\n",
      "Accuracy: 70.3%\n",
      "model:  RetrieveOnly\n",
      "Accuracy: 82.3%\n",
      "model:  StyleEmbedding\n",
      "Accuracy: 42.4%\n",
      "model:  TemplateBased\n",
      "Accuracy: 64.8%\n",
      "model:  B_GST\n",
      "Accuracy: 59.099999999999994%\n",
      "model:  G_GST\n",
      "Accuracy: 57.3%\n",
      "model:  DeleteOnly\n",
      "Accuracy: 50.0%\n",
      "model:  input_copy\n",
      "Accuracy: 18.5%\n"
     ]
    }
   ],
   "source": [
    "### classifier accuracy\n",
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_correct = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        \n",
    "        token_idx = torch.tensor(gpt_tokenizer.encode(out_sen)).unsqueeze(0).cuda()\n",
    "\n",
    "        \"\"\"discriminator\"\"\"\n",
    "        if token_idx.shape[1] != 0:\n",
    "            result = dismodel.discriminator(token_idx=token_idx).argmax(1).cpu().numpy()[0]    \n",
    "            if result == 1: ## style transfer so result must be 1(positive)\n",
    "                neg_correct += 1\n",
    "            \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_correct = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        \n",
    "        token_idx = torch.tensor(gpt_tokenizer.encode(out_sen)).unsqueeze(0).cuda()\n",
    "\n",
    "        \"\"\"discriminator\"\"\"\n",
    "        if token_idx.shape[1] != 0:\n",
    "            result = dismodel.discriminator(token_idx=token_idx).argmax(1).cpu().numpy()[0]    \n",
    "            if result == 0: ## style transfer so result must be 0(negative)\n",
    "                pos_correct += 1\n",
    "    \n",
    "    Acc = (neg_correct+pos_correct)/(neg_len+pos_len)*100\n",
    "    print(\"Accuracy: {}%\".format(Acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0e0145a44d41b5a5046567d1cb83a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f68d7a3fede4f7eb82acf668e9479dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.68 seconds, 271.41 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d79820954894314bc21a0cbb9e76bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5c9c91ab414d04a4c50925fd91b50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.71 seconds, 269.20 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91b38fe0d23426db2b86d884ba4fc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef48adda9b44412a92e6477cb2bd6ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 1.98 seconds, 504.00 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f308a9c635471fb1133f1fedd2678f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a26843d84934773865d92298da4446b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.68 seconds, 271.50 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d806231af504e9e92fc2ce444e4a9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81767d7cafc74e758bae044d50ef2ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 4.45 seconds, 224.61 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b477672328e42de9ec35cf74cc60af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d353cbc1b734e15b2488c08a5b469a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.68 seconds, 272.09 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a981a5191b3a49d980e69786933b4b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1896626a3c2434699acf9ec6ff12c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.80 seconds, 263.42 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b7769aa2b541eabe48120c19eb8c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c73823acaaf43659090dedb1725288b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.75 seconds, 266.59 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a82b4cac50429aa5a3aeac645c6122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c21696da9e4843b4c811d1bcb0dbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.76 seconds, 266.21 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9245ef2cdd4c6985cd55e2ae30b2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a8a38417ed47b7b10cc82bf3300632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.96 seconds, 252.62 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8514cbaee24be4b454b7c6f154b5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f8cd744b484d1091283b788dea6442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.59 seconds, 278.42 sentences/sec\n",
      "model:  CrossAligned\n",
      "BERT score(F1): 85.94848718047142\n",
      "\n",
      "model:  DeleteAndRetrieve\n",
      "BERT score(F1): 90.92191798090936\n",
      "\n",
      "model:  human\n",
      "BERT score(F1): 100.00000314712526\n",
      "\n",
      "model:  multi_decoder\n",
      "BERT score(F1): 86.08985497355461\n",
      "\n",
      "model:  RetrieveOnly\n",
      "BERT score(F1): 85.53820195198058\n",
      "\n",
      "model:  StyleEmbedding\n",
      "BERT score(F1): 87.39492159485816\n",
      "\n",
      "model:  TemplateBased\n",
      "BERT score(F1): 90.65091289877891\n",
      "\n",
      "model:  B_GST\n",
      "BERT score(F1): 91.22536595463752\n",
      "\n",
      "model:  G_GST\n",
      "BERT score(F1): 89.47910848855972\n",
      "\n",
      "model:  DeleteOnly\n",
      "BERT score(F1): 90.54864029884338\n",
      "\n",
      "model:  input_copy\n",
      "BERT score(F1): 93.7680603146553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/amazon/compare1/amazon/'\n",
    "neg_human = human_path + 'sentiment.test.0.human'\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = human_path + 'sentiment.test.1.human'\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "human_refs = []\n",
    "neg_len = len(neg_human_dataset)\n",
    "pos_len = len(pos_human_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_dataset[k].split('\\t')[1]\n",
    "    human_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_dataset[k].split('\\t')[1]\n",
    "    human_refs.append(ref_sen)\n",
    "    \n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "model_name=[]\n",
    "score_list=[]\n",
    "for i in range(len(neg_out_files)):    \n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, human_F1 = score(cands, human_refs, lang='en', verbose=True)\n",
    "    human_F1_list=list(human_F1.numpy())\n",
    "    human_BERT_scroe = sum(human_F1_list)/len(human_F1_list)\n",
    "    \n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(human_BERT_scroe)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80559d096ae24dd88b6afa863183580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baaaf2b6ae44051841f0ae91a5f2184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.74 seconds, 267.24 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47718a3e44b8435997b6275e2bc66606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732f7cdc9eef484c8e168c3cd83b1bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.91 seconds, 255.92 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac0498639d240c887c65bc25502920a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80386bf26f34b5888da1b98b45665ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.71 seconds, 269.63 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ac9aad99ae447c9c1e3137e9463843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72e53b56a5b44c3885c71df50518cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.48 seconds, 287.13 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b30a1a93e114906ab12000b89e5a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ceaa3e8c43492595eacea41540d0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 4.71 seconds, 212.13 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedd16d418114999bdfa3aa32155afa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3d3b1ac4274831a5fdee38727d6435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.82 seconds, 261.97 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a07aa1190a43af8ffabe405c064b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62999560f5dd48de9cde463d24bef4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.93 seconds, 254.74 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a59b5c1dfaf4ef5978982ad14355644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8470e30598e4c42a07fb5f4fb87be8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.90 seconds, 256.13 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2172a76c8434400491159f9896918408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5978f61d3eee42b0bb218b5b2bdf4360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.69 seconds, 270.91 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b416873e84954f879db8273cecbe3f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09d41f6c366450ca06a9266568013a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 3.63 seconds, 275.78 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57ac953e25843569c97a28f13a9f97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8198f4f271409182f468ab30c845f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done in 2.27 seconds, 439.59 sentences/sec\n",
      "model:  CrossAligned\n",
      "BERT score(F1): 86.51291912198067\n",
      "\n",
      "model:  DeleteAndRetrieve\n",
      "BERT score(F1): 94.41067969799042\n",
      "\n",
      "model:  human\n",
      "BERT score(F1): 93.7680603146553\n",
      "\n",
      "model:  multi_decoder\n",
      "BERT score(F1): 87.32869731783866\n",
      "\n",
      "model:  RetrieveOnly\n",
      "BERT score(F1): 86.47710822224617\n",
      "\n",
      "model:  StyleEmbedding\n",
      "BERT score(F1): 89.39386991858483\n",
      "\n",
      "model:  TemplateBased\n",
      "BERT score(F1): 94.7468332350254\n",
      "\n",
      "model:  B_GST\n",
      "BERT score(F1): 94.34757762551308\n",
      "\n",
      "model:  G_GST\n",
      "BERT score(F1): 92.2913361787796\n",
      "\n",
      "model:  DeleteOnly\n",
      "BERT score(F1): 93.90700323581696\n",
      "\n",
      "model:  input_copy\n",
      "BERT score(F1): 100.00000414848327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### BERT score with self-reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "neg_human = neg_out_files[0]\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = pos_out_files[0]\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "refs = []\n",
    "neg_len = len(neg_human_dataset)\n",
    "pos_len = len(pos_human_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_dataset[k].split('\\t')[0]\n",
    "    refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_dataset[k].split('\\t')[0]\n",
    "    refs.append(ref_sen)\n",
    "        \n",
    "model_name=[]\n",
    "score_list=[]    \n",
    "for i in range(len(neg_out_files)):\n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(BERT_score)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "gpt2_lm_tokenizer = tokenizer_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model = model_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model.to(device)\n",
    "gpt2_lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = gpt2_lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = gpt2_lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  CrossAligned\n",
      "PPL: 119.36589266586304\n",
      "model:  DeleteAndRetrieve\n",
      "PPL: 221.91834246635437\n",
      "model:  human\n",
      "PPL: 132.1787586941719\n",
      "model:  multi_decoder\n",
      "PPL: 343.72147521686554\n",
      "model:  RetrieveOnly\n",
      "PPL: 135.21915171718598\n",
      "model:  StyleEmbedding\n",
      "PPL: 369.2391248292923\n",
      "model:  TemplateBased\n",
      "PPL: 368.4148586606979\n",
      "model:  B_GST\n",
      "PPL: 193.7342904434204\n",
      "model:  G_GST\n",
      "PPL: 458.9293729791641\n",
      "model:  DeleteOnly\n",
      "PPL: 251.23735857009888\n",
      "model:  input_copy\n",
      "PPL: 188.33442417812347\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "path = '/DATA/joosung/controllable_english/gpt2/finetune_amazon/final/pytorch_model.bin'\n",
    "lm_model_state_dict = torch.load(path)\n",
    "lm_model.load_state_dict(lm_model_state_dict)\n",
    "lm_model.to(device)\n",
    "lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt_yelp(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  CrossAligned\n",
      "PPL: 1.1097679322957992\n",
      "model:  DeleteAndRetrieve\n",
      "PPL: 2.433586782217026\n",
      "model:  human\n",
      "PPL: 12.376503941774368\n",
      "model:  multi_decoder\n",
      "PPL: 1.3934451076984407\n",
      "model:  RetrieveOnly\n",
      "PPL: 5.6538989824056625\n",
      "model:  StyleEmbedding\n",
      "PPL: 3.421233607172966\n",
      "model:  TemplateBased\n",
      "PPL: 5.359758041858673\n",
      "model:  B_GST\n",
      "PPL: 12448.444616994262\n",
      "model:  G_GST\n",
      "PPL: 18105.997926429867\n",
      "model:  DeleteOnly\n",
      "PPL: 2.783881105542183\n",
      "model:  input_copy\n",
      "PPL: 3.7645388329029084\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final\n",
    "model:  CrossAligned\n",
    "PPL: 1.1097679322957992\n",
    "model:  DeleteAndRetrieve\n",
    "PPL: 2.433586782217026\n",
    "model:  human\n",
    "PPL: 12.376503941774368\n",
    "model:  multi_decoder\n",
    "PPL: 1.3934451076984407\n",
    "model:  RetrieveOnly\n",
    "PPL: 5.6538989824056625\n",
    "model:  StyleEmbedding\n",
    "PPL: 3.421233607172966\n",
    "model:  TemplateBased\n",
    "PPL: 5.359758041858673\n",
    "model:  B_GST\n",
    "PPL: 12448.444616994262\n",
    "model:  G_GST\n",
    "PPL: 18105.997926429867\n",
    "model:  DeleteOnly\n",
    "PPL: 2.783881105542183\n",
    "model:  input_copy\n",
    "PPL: 3.7645388329029084\n",
    "\n",
    "# 5\n",
    "model:  CrossAligned\n",
    "PPL: 1.1097679322957992\n",
    "model:  DeleteAndRetrieve\n",
    "PPL: 2.433586782217026\n",
    "model:  human\n",
    "PPL: 12.376503941774368\n",
    "model:  multi_decoder\n",
    "PPL: 1.3934451076984407\n",
    "model:  RetrieveOnly\n",
    "PPL: 5.6538989824056625\n",
    "model:  StyleEmbedding\n",
    "PPL: 3.421233607172966\n",
    "model:  TemplateBased\n",
    "PPL: 5.359758041858673\n",
    "model:  B_GST\n",
    "PPL: 12448.444616994262\n",
    "model:  G_GST\n",
    "PPL: 18105.997926429867\n",
    "model:  DeleteOnly\n",
    "PPL: 2.783881105542183\n",
    "model:  input_copy\n",
    "PPL: 3.7645388329029084\n",
    "\n",
    "# 3 \n",
    "model:  CrossAligned\n",
    "PPL: 1.1097679322957992\n",
    "model:  DeleteAndRetrieve\n",
    "PPL: 2.433586782217026\n",
    "model:  human\n",
    "PPL: 12.376503941774368\n",
    "model:  multi_decoder\n",
    "PPL: 1.3934451076984407\n",
    "model:  RetrieveOnly\n",
    "PPL: 5.6538989824056625\n",
    "model:  StyleEmbedding\n",
    "PPL: 3.421233607172966\n",
    "model:  TemplateBased\n",
    "PPL: 5.359758041858673\n",
    "model:  B_GST\n",
    "PPL: 12448.444616994262\n",
    "model:  G_GST\n",
    "PPL: 18105.997926429867\n",
    "model:  DeleteOnly\n",
    "PPL: 2.783881105542183\n",
    "model:  input_copy\n",
    "PPL: 3.7645388329029084\n",
    "\n",
    "# 2    \n",
    "model:  CrossAligned\n",
    "PPL: 1.1478351253271102\n",
    "model:  DeleteAndRetrieve\n",
    "PPL: 3.0744097059965134\n",
    "model:  human\n",
    "PPL: 17.726013666272163\n",
    "model:  multi_decoder\n",
    "PPL: 1.6406387261152267\n",
    "model:  RetrieveOnly\n",
    "PPL: 6.3233569920063015\n",
    "model:  StyleEmbedding\n",
    "PPL: 3.917377111792564\n",
    "model:  TemplateBased\n",
    "PPL: 6.016205228924751\n",
    "model:  B_GST\n",
    "PPL: 204.93594883191585\n",
    "model:  G_GST\n",
    "PPL: 792.7048557529449\n",
    "model:  DeleteOnly\n",
    "PPL: 3.3367079955339434\n",
    "model:  input_copy\n",
    "PPL: 4.469998460769653\n",
    "    \n",
    "# 1\n",
    "model:  CrossAligned\n",
    "PPL: 19.397055559396744\n",
    "model:  DeleteAndRetrieve\n",
    "PPL: 99.03166322278976\n",
    "model:  human\n",
    "PPL: 590.6161570885181\n",
    "model:  multi_decoder\n",
    "PPL: 79.88124821519851\n",
    "model:  RetrieveOnly\n",
    "PPL: 155.75980024719237\n",
    "model:  StyleEmbedding\n",
    "PPL: 91.11133183479309\n",
    "model:  TemplateBased\n",
    "PPL: 241.68037927341462\n",
    "model:  B_GST\n",
    "PPL: 361.3149083812237\n",
    "model:  G_GST\n",
    "PPL: 686.4326925992966\n",
    "model:  DeleteOnly\n",
    "PPL: 116.11603822183609\n",
    "model:  input_copy\n",
    "PPL: 170.48672652459143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  B_GST\n",
      "PPL: 12448.444616994262\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*0.B_GST*')\n",
    "pos_out_files = glob.glob(output_path + '*1.B_GST*')\n",
    "neg_list = []\n",
    "pos_list = []\n",
    "neg_sen = []\n",
    "pos_sen = []\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "            neg_list.append(sample_PPL)\n",
    "            if sample_PPL > 100:\n",
    "                neg_sen.append([out_sen])\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "            pos_list.append(sample_PPL)\n",
    "            if sample_PPL > 100:\n",
    "                pos_sen.append([out_sen])\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['another num _ num calories from the num _ num of bacon .'],\n",
       " ['great product ; i purchased this from a major retailer about num _ num years ago .'],\n",
       " ['the first time , things work really fast down around level num _ extend'],\n",
       " ['we ve used num _ num of these waffle makers since jan num _ extend'],\n",
       " ['i would not buy anything else other than num _ extend'],\n",
       " ['most operations there is a noticable charge of num _ extend to num _ num hours .'],\n",
       " ['great for birds or squirrels . num _ num kinds of bird .'],\n",
       " ['i got this for my iphone num _ num year old for christmas .'],\n",
       " ['im num _ num gem in quality at least a num _ num inch .'],\n",
       " ['in num _ num minutes num _ extend thq came with this charger .'],\n",
       " ['it s a great case out of a num _ num num _ extend'],\n",
       " ['i did buy mine from a num _ extend seller .'],\n",
       " ['my daughter used this for school for about num _ num months until the phone broke .'],\n",
       " ['i have num _ num bird feeders from poles in my front yard .'],\n",
       " ['only thing for num _ num gallon lids are cheap .'],\n",
       " ['i was shocked after the first num _ num months .'],\n",
       " ['i am only giving this unit num _ num stars because of the price .'],\n",
       " ['soe decided to push it live a whole num _ num minutes than that .'],\n",
       " ['i dumped about num _ num in the trash bin .'],\n",
       " ['i purchased this along with another similar case , for over num _ num months .'],\n",
       " ['it lasts for maybe num _ num minutes then dies .'],\n",
       " ['nevertheless , the size works great for num _ num or num _ num tops .'],\n",
       " ['the plantronics radtach procable num _ num inch battery only part way .'],\n",
       " ['i ordered num _ num and find this one to be a great deal .'],\n",
       " ['in this case , it s a big plus for num _ extend'],\n",
       " ['i bought this to hold my num _ num persian cats while on vacation .'],\n",
       " ['as the clock num _ extend i still felt disconnected .']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
