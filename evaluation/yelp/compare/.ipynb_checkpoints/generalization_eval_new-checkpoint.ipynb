{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "output_path = '/DATA/joosung/controllable_english/evaluation/yelp/compare/yelp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  B_GST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 43.445969010960475\n",
      "model:  CrossAligned\n",
      "BLEU score: 17.015450645847654\n",
      "model:  DeleteAndRetrieve\n",
      "BLEU score: 34.47547504586853\n",
      "model:  DeleteOnly\n",
      "BLEU score: 33.93628403240791\n",
      "model:  G_GST\n",
      "BLEU score: 43.94119083533587\n",
      "model:  humanDRG\n",
      "BLEU score: 26.9693395196322\n",
      "model:  humanDUAL\n",
      "BLEU score: 36.79476631343228\n",
      "model:  input_copy\n",
      "BLEU score: 100.0\n",
      "model:  multi_decoder\n",
      "BLEU score: 40.80852135369808\n",
      "model:  RetrieveOnly\n",
      "BLEU score: 0.880880862993207\n",
      "model:  StyleEmbedding\n",
      "BLEU score: 71.80395314507336\n",
      "model:  TemplateBased\n",
      "BLEU score: 48.674772350951216\n",
      "model:  BackTranslation\n",
      "BLEU score: 0.6682748635680508\n",
      "model:  DualRL\n",
      "BLEU score: 58.72439096444732\n",
      "model:  UnpairedRL\n",
      "BLEU score: 42.28547851663075\n"
     ]
    }
   ],
   "source": [
    "### self-BLEU\n",
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_bleu_1 = 0\n",
    "    neg_bleu_2 = 0\n",
    "    neg_bleu_3 = 0\n",
    "    neg_bleu_4 = 0\n",
    "    neg_bleu_score = 0\n",
    "    for k in range(neg_len):\n",
    "        ori_sen = neg_data_dataset[k].split('\\t')[0]\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_1 += bleu_1_score\n",
    "        neg_bleu_2 += bleu_2_score\n",
    "        neg_bleu_3 += bleu_3_score\n",
    "        neg_bleu_4 += bleu_4_score\n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_bleu_1 = 0\n",
    "    pos_bleu_2 = 0\n",
    "    pos_bleu_3 = 0\n",
    "    pos_bleu_4 = 0\n",
    "    pos_bleu_score = 0\n",
    "    for k in range(pos_len):\n",
    "        ori_sen = pos_data_dataset[k].split('\\t')[0]\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_1 += bleu_1_score\n",
    "        pos_bleu_2 += bleu_2_score\n",
    "        pos_bleu_3 += bleu_3_score\n",
    "        pos_bleu_4 += bleu_4_score\n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    bleu_1 = (neg_bleu_1+pos_bleu_1)/(neg_len+pos_len)\n",
    "    bleu_2 = (neg_bleu_2+pos_bleu_2)/(neg_len+pos_len)\n",
    "    bleu_3 = (neg_bleu_3+pos_bleu_3)/(neg_len+pos_len)\n",
    "    bleu_4 = (neg_bleu_4+pos_bleu_4)/(neg_len+pos_len)\n",
    "    blue_score = (neg_bleu_score+pos_bleu_score)/(neg_len+pos_len)\n",
    "#     print('BLEU_1 score: {}'.format(bleu_1))\n",
    "#     print('BLEU_2 score: {}'.format(bleu_2))\n",
    "#     print('BLEU_3 score: {}'.format(bleu_3))\n",
    "#     print('BLEU_4 score: {}'.format(bleu_4))\n",
    "#     print('Average: ', (bleu_1+bleu_2+bleu_3+bleu_4)/4*100)\n",
    "    print('BLEU score: {}'.format(blue_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  B_GST\n",
      "BLEU score: 13.492065562759128\n",
      "model:  CrossAligned\n",
      "BLEU score: 4.336741798013845\n",
      "model:  DeleteAndRetrieve\n",
      "BLEU score: 9.823532194540812\n",
      "model:  DeleteOnly\n",
      "BLEU score: 9.28509410000901\n",
      "model:  G_GST\n",
      "BLEU score: 13.277896101990041\n",
      "model:  humanDRG\n",
      "BLEU score: 53.3542075695261\n",
      "model:  humanDUAL\n",
      "BLEU score: 33.01971646649611\n",
      "model:  input_copy\n",
      "BLEU score: 21.014577208796236\n",
      "model:  multi_decoder\n",
      "BLEU score: 8.237610334097875\n",
      "model:  RetrieveOnly\n",
      "BLEU score: 0.42682838584074756\n",
      "model:  StyleEmbedding\n",
      "BLEU score: 13.647917881993484\n",
      "model:  TemplateBased\n",
      "BLEU score: 12.859593684528473\n",
      "model:  BackTranslation\n",
      "BLEU score: 0.5202224657007146\n",
      "model:  DualRL\n",
      "BLEU score: 17.709507727994133\n",
      "model:  UnpairedRL\n",
      "BLEU score: 10.601903591684167\n"
     ]
    }
   ],
   "source": [
    "### human-BLEU\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/yelp/human/'\n",
    "neg_human_DRG = human_path + 'sentiment.test.0.humanDRG'\n",
    "neg_human_DRG_open = open(neg_human_DRG, \"r\")\n",
    "neg_human_DRG_dataset = neg_human_DRG_open.readlines()\n",
    "neg_human_DRG_open.close()\n",
    "\n",
    "pos_human_DRG = human_path + 'sentiment.test.1.humanDRG'\n",
    "pos_human_DRG_open = open(pos_human_DRG, \"r\")\n",
    "pos_human_DRG_dataset = pos_human_DRG_open.readlines()\n",
    "pos_human_DRG_open.close()\n",
    "\n",
    "neg_human_DUAL = human_path + 'sentiment.test.0.humanDUAL'\n",
    "neg_human_DUAL_open = open(neg_human_DUAL, \"r\")\n",
    "neg_human_DUAL_dataset = neg_human_DUAL_open.readlines()\n",
    "neg_human_DUAL_open.close()\n",
    "\n",
    "pos_human_DUAL = human_path + 'sentiment.test.0.humanDUAL'\n",
    "pos_human_DUAL_open = open(pos_human_DUAL, \"r\")\n",
    "pos_human_DUAL_dataset = pos_human_DUAL_open.readlines()\n",
    "pos_human_DUAL_open.close()\n",
    "\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_bleu_score = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        \n",
    "        ### human DRG \n",
    "        ori_sen = neg_human_DRG_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)        \n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "        \n",
    "        ### human DUAL\n",
    "        ori_sen = neg_human_DUAL_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        neg_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_bleu_score = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        out_tokens=word_tokenize(out_sen)\n",
    "        \n",
    "        ### human DRG \n",
    "        ori_sen = pos_human_DRG_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)        \n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "        \n",
    "        ### human DUAL\n",
    "        ori_sen = pos_human_DUAL_dataset[k].split('\\t')[1]        \n",
    "        ori_tokens=word_tokenize(ori_sen)\n",
    "        ori_tokens = [ori_tokens]\n",
    "        \n",
    "        bleu_1_score = sentence_bleu(ori_tokens, out_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4_score = sentence_bleu(ori_tokens, out_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        pos_bleu_score += min(1, len(out_tokens)/len(ori_tokens))*((bleu_1_score*bleu_2_score*bleu_3_score*bleu_4_score)**(0.25))\n",
    "    \n",
    "    blue_score = (neg_bleu_score+pos_bleu_score)/((neg_len+pos_len)*2)\n",
    "    print('BLEU score: {}'.format(blue_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import *\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "import json\n",
    "f = open('../gpt_yelp_vocab.json')\n",
    "token2num = json.load(f)\n",
    "\n",
    "num2token = {}\n",
    "for key, value in token2num.items():\n",
    "    num2token[value] = key\n",
    "import sys    \n",
    "sys.path.insert(0, \"/DATA/joosung/controllable_english/yelp/visual_v3_1\")\n",
    "from dis_model import *\n",
    "dismodel = findattribute().cuda()\n",
    "\n",
    "i='final'\n",
    "dismodel_name='cls_model_' + str(i)\n",
    "dismodel.load_state_dict(torch.load('/DATA/joosung/controllable_english/yelp/visual_v3_1/models/{}'.format(dismodel_name)))\n",
    "dismodel.eval()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  B_GST\n"
     ]
    }
   ],
   "source": [
    "### classifier accuracy\n",
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_correct = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        \n",
    "        token_idx = torch.tensor(gpt_tokenizer.encode(out_sen)).unsqueeze(0).cuda()\n",
    "\n",
    "        \"\"\"discriminator\"\"\"\n",
    "        if token_idx.shape[1] != 0:\n",
    "            result = dismodel.discriminator(token_idx=token_idx).argmax(1).cpu().numpy()[0]    \n",
    "            if result == 1: ## style transfer so result must be 1(positive)\n",
    "                neg_correct += 1\n",
    "            \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_correct = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        \n",
    "        token_idx = torch.tensor(gpt_tokenizer.encode(out_sen)).unsqueeze(0).cuda()\n",
    "\n",
    "        \"\"\"discriminator\"\"\"\n",
    "        if token_idx.shape[1] != 0:\n",
    "            result = dismodel.discriminator(token_idx=token_idx).argmax(1).cpu().numpy()[0]    \n",
    "            if result == 0: ## style transfer so result must be 0(negative)\n",
    "                pos_correct += 1\n",
    "    \n",
    "    Acc = (neg_correct+pos_correct)/(neg_len+pos_len)*100\n",
    "    print(\"Accuracy: {}%\".format(Acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with human reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "human_path = '/DATA/joosung/controllable_english/evaluation/yelp/compare1/yelp/'\n",
    "neg_human_DRG = human_path + 'sentiment.test.0.humanDRG'\n",
    "neg_human_DRG_open = open(neg_human_DRG, \"r\")\n",
    "neg_human_DRG_dataset = neg_human_DRG_open.readlines()\n",
    "neg_human_DRG_open.close()\n",
    "\n",
    "pos_human_DRG = human_path + 'sentiment.test.1.humanDRG'\n",
    "pos_human_DRG_open = open(pos_human_DRG, \"r\")\n",
    "pos_human_DRG_dataset = pos_human_DRG_open.readlines()\n",
    "pos_human_DRG_open.close()\n",
    "\n",
    "neg_human_DUAL = human_path + 'sentiment.test.0.humanDUAL'\n",
    "neg_human_DUAL_open = open(neg_human_DUAL, \"r\")\n",
    "neg_human_DUAL_dataset = neg_human_DUAL_open.readlines()\n",
    "neg_human_DUAL_open.close()\n",
    "\n",
    "pos_human_DUAL = human_path + 'sentiment.test.1.humanDUAL'\n",
    "pos_human_DUAL_open = open(pos_human_DUAL, \"r\")\n",
    "pos_human_DUAL_dataset = pos_human_DUAL_open.readlines()\n",
    "pos_human_DUAL_open.close()\n",
    "\n",
    "DRG_refs = []\n",
    "neg_len = len(neg_human_DRG_dataset)\n",
    "pos_len = len(pos_human_DRG_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_DRG_dataset[k].split('\\t')[1]\n",
    "    DRG_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_DRG_dataset[k].split('\\t')[1]\n",
    "    DRG_refs.append(ref_sen)\n",
    "\n",
    "DUAL_refs = []\n",
    "neg_len = len(neg_human_DUAL_dataset)\n",
    "pos_len = len(pos_human_DUAL_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_DUAL_dataset[k].split('\\t')[1]\n",
    "    DUAL_refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_DUAL_dataset[k].split('\\t')[1]\n",
    "    DUAL_refs.append(ref_sen)\n",
    "    \n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "model_name=[]\n",
    "score_list=[]\n",
    "for i in range(len(neg_out_files)):    \n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, DRG_F1 = score(cands, DRG_refs, lang='en', verbose=True)\n",
    "    DRG_F1_list=list(DRG_F1.numpy())\n",
    "    DRG_BERT_scroe = sum(DRG_F1_list)/len(DRG_F1_list)\n",
    "    \n",
    "    P, R, DUAL_F1 = score(cands, DUAL_refs, lang='en', verbose=True)\n",
    "    DUAL_F1_list=list(DUAL_F1.numpy())\n",
    "    DUAL_BERT_scroe = sum(DUAL_F1_list)/len(DUAL_F1_list)\n",
    "    \n",
    "    BERT_score = (DRG_BERT_scroe+DUAL_BERT_scroe)/2    \n",
    "    \n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(BERT_score)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT score with self-reference\n",
    "from bert_score import score\n",
    "import glob\n",
    "\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "\n",
    "neg_human = neg_out_files[0]\n",
    "neg_human_open = open(neg_human, \"r\")\n",
    "neg_human_dataset = neg_human_open.readlines()\n",
    "neg_human_open.close()\n",
    "\n",
    "pos_human = pos_out_files[0]\n",
    "pos_human_open = open(pos_human, \"r\")\n",
    "pos_human_dataset = pos_human_open.readlines()\n",
    "pos_human_open.close()\n",
    "\n",
    "refs = []\n",
    "neg_len = len(neg_human_dataset)\n",
    "pos_len = len(pos_human_dataset)\n",
    "for k in range(neg_len):\n",
    "    ref_sen = neg_human_dataset[k].split('\\t')[0]\n",
    "    refs.append(ref_sen)\n",
    "for k in range(pos_len):\n",
    "    ref_sen = pos_human_dataset[k].split('\\t')[0]\n",
    "    refs.append(ref_sen)\n",
    "        \n",
    "model_name=[]\n",
    "score_list=[]    \n",
    "for i in range(len(neg_out_files)):\n",
    "    cands = []\n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1]\n",
    "        cands.append(out_sen)\n",
    "\n",
    "    P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "    F1_list=list(F1.numpy())\n",
    "    BERT_score = sum(F1_list)/len(F1_list)\n",
    "    model_name.append(neg_out_files[i].split('.')[-1])\n",
    "    score_list.append(BERT_score)\n",
    "for i in range(len(model_name)):\n",
    "    print(\"model: \", model_name[i])\n",
    "    print('BERT score(F1): {}'.format(score_list[i]*100))\n",
    "    print('')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "model_class, tokenizer_class = (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "gpt2_lm_tokenizer = tokenizer_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model = model_class.from_pretrained('gpt2-large')\n",
    "gpt2_lm_model.to(device)\n",
    "gpt2_lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = gpt2_lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = gpt2_lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PPL\n",
    "import torch, os\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "path = '/DATA/joosung/controllable_english/gpt2/finetune_yelp/final/pytorch_model.bin'\n",
    "lm_model_state_dict = torch.load(path)\n",
    "lm_model.load_state_dict(lm_model_state_dict)\n",
    "lm_model.to(device)\n",
    "lm_model.eval()\n",
    "\n",
    "lm_loss = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def calculate_ppl_gpt_yelp(sentence_batch):\n",
    "    # tokenize the sentences\n",
    "    tokenized_ids = [None for i in range(len(sentence_batch))]\n",
    "    \n",
    "    for i in range(len(sentence_batch)):\n",
    "        tokenized_ids[i] = lm_tokenizer.encode(sentence_batch[i])\n",
    "        \n",
    "    sen_lengths = [len(x) for x in tokenized_ids]\n",
    "    max_sen_lenght = max(sen_lengths)\n",
    "    \n",
    "    n_batch = len(sentence_batch)\n",
    "    input_ids = np.zeros( shape=(n_batch, max_sen_lenght), dtype=np.int64)\n",
    "    lm_labels = np.full(shape=(n_batch, max_sen_lenght), fill_value=-1)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_ids):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "        lm_labels[i, :len(tokens)-1] = tokens[1:] \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    lm_labels = torch.tensor(lm_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "        lm_pred = lm_model(input_ids)\n",
    "    loss_val = lm_loss(lm_pred[0].view(-1, lm_pred[0].size(-1)), lm_labels.view(-1))\n",
    "    normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1) / torch.tensor(sen_lengths, dtype=torch.float32).to(device)\n",
    "    #normalized_loss = loss_val.view(n_batch,-1).sum(dim= -1)\n",
    "    ppl = torch.exp(normalized_loss)\n",
    "    return  ppl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "neg_out_files = glob.glob(output_path + '*test.0*')\n",
    "pos_out_files = glob.glob(output_path + '*test.1*')\n",
    "for i in range(len(neg_out_files)):\n",
    "    print(\"model: \", neg_out_files[i].split('.')[-1])\n",
    "    \n",
    "    neg_data_open = open(neg_out_files[i], \"r\")\n",
    "    neg_data_dataset = neg_data_open.readlines()\n",
    "    neg_len = len(neg_data_dataset)\n",
    "    neg_data_open.close()\n",
    "    \n",
    "    neg_PPL = 0\n",
    "    for k in range(neg_len):\n",
    "        out_sen = neg_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            neg_PPL += sample_PPL\n",
    "    \n",
    "    pos_out_name = neg_out_files[i].split('.0.')[0]+'.1.'+neg_out_files[i].split('.0.')[-1]\n",
    "    pos_data_open = open(pos_out_name, \"r\")\n",
    "    pos_data_dataset = pos_data_open.readlines()\n",
    "    pos_len = len(neg_data_dataset)\n",
    "    pos_data_open.close()\n",
    "    \n",
    "    pos_PPL = 0\n",
    "    for k in range(pos_len):\n",
    "        out_sen = pos_data_dataset[k].split('\\t')[1].strip()\n",
    "        if len(out_sen) != 0:\n",
    "            sample_PPL = calculate_ppl_gpt_yelp([out_sen])[0]\n",
    "            pos_PPL += sample_PPL\n",
    "\n",
    "    total_PPL = (neg_PPL+pos_PPL)/(neg_len+pos_len)\n",
    "    print('PPL: {}'.format(total_PPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
