{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('gpt_yelp_vocab.json')\n",
    "token2num = json.load(f)\n",
    "\n",
    "num2token = {}\n",
    "for key, value in token2num.items():\n",
    "    num2token[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "findattribute(\n",
       "  (word_emb): Embedding(50259, 256, padding_idx=50258)\n",
       "  (conv2d_2): Conv2d(1, 100, kernel_size=(2, 256), stride=(1, 1))\n",
       "  (conv2d_3): Conv2d(1, 100, kernel_size=(3, 256), stride=(1, 1))\n",
       "  (conv2d_4): Conv2d(1, 100, kernel_size=(4, 256), stride=(1, 1))\n",
       "  (conv2d_5): Conv2d(1, 100, kernel_size=(5, 256), stride=(1, 1))\n",
       "  (disc_fc): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/DATA/joosung/controllable_english/yelp/classifier/\")\n",
    "from dis_model import *\n",
    "dismodel = findattribute().cuda()\n",
    "dismodel_name='cls_model_3'\n",
    "dismodel.load_state_dict(torch.load('../classifier/models/{}'.format(dismodel_name)))\n",
    "dismodel.eval()\n",
    "\n",
    "# from gen_model import *\n",
    "# genmodel = styletransfer().cuda()\n",
    "# genmodel_name='gen_model_3'\n",
    "# genmodel.load_state_dict(torch.load('./models/{}'.format(genmodel_name)))\n",
    "# genmodel.eval()\n",
    "# print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "data_path = \"/DATA/joosung/sentiment_data/Sentiment-and-Style-Transfer-master/data\"\n",
    "yelp_neg_path = data_path + \"/yelp/sentiment.test.0\"\n",
    "yelp_neg_open = open(yelp_neg_path, \"r\")\n",
    "yelp_neg_dataset = yelp_neg_open.readlines()\n",
    "neg_len = len(yelp_neg_dataset)\n",
    "yelp_neg_open.close()\n",
    "\n",
    "yelp_pos_path = data_path + \"/yelp/sentiment.test.1\"\n",
    "yelp_pos_open = open(yelp_pos_path, \"r\")\n",
    "yelp_pos_dataset = yelp_pos_open.readlines()\n",
    "pos_len = len(yelp_pos_dataset)\n",
    "yelp_pos_open.close()\n",
    "\n",
    "stop_point = 30\n",
    "# stop_point = pos_len*epoch+batch\n",
    "\n",
    "PAD_IDX = 50258\n",
    "\n",
    "for start in range(stop_point-1, stop_point):\n",
    "    \"\"\"data start point\"\"\"\n",
    "    neg_start = start%neg_len\n",
    "    pos_start = start%pos_len\n",
    "    \n",
    "    \"\"\"data setting\"\"\"\n",
    "    neg_sentence = yelp_neg_dataset[neg_start].strip()\n",
    "    pos_sentence = yelp_pos_dataset[pos_start].strip()                \n",
    "        \n",
    "    neg_labels = [] # negative labels\n",
    "    neg_labels.append([1,0])\n",
    "    neg_attribute = torch.from_numpy(np.asarray(neg_labels)).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    pos_labels = [] # positive labels\n",
    "    pos_labels.append([0,1])\n",
    "    pos_attribute = torch.from_numpy(np.asarray(pos_labels)).type(torch.FloatTensor).cuda()\n",
    "\n",
    "    sentences = [neg_sentence, pos_sentence]\n",
    "    attributes = [neg_attribute, pos_attribute]\n",
    "    fake_attributes = [pos_attribute, neg_attribute]\n",
    "    sentiments = [0, 1]\n",
    "    \"\"\"data input\"\"\"\n",
    "    for i in range(2):\n",
    "        # k=0: negative, k=1: positive\n",
    "        sentence = sentences[i]        \n",
    "        for k in range(6):\n",
    "            fake_attribute = k/5*attributes[0] + (1-k/5)*attributes[1]            \n",
    "#             attribute = attributes[i] # for decoder\n",
    "#             fake_attribute = attributes[abs(1-i)] # for generate            \n",
    "\n",
    "            token_idx = torch.tensor(gpt_tokenizer.encode(sentence)).unsqueeze(0).cuda()\n",
    "            ori_length = token_idx.shape[1]\n",
    "\n",
    "            # delete model\n",
    "            max_len = int(token_idx.shape[1]/2)\n",
    "            sentiment = sentiments[i] # for delete\n",
    "#             sentiment = dis_out.argmax(1).cpu().item() ## 변경점 for delete\n",
    "#             dis_out = dismodel.discriminator(token_idx)                \n",
    "\n",
    "            del_idx = token_idx\n",
    "            for k in range(max_len):\n",
    "                del_idx = dismodel.att_prob(del_idx, sentiment)           \n",
    "                dis_out = dismodel.discriminator(del_idx)    \n",
    "                sent_porb = F.softmax(dis_out, 1).squeeze(0)[sentiment].cpu().detach().numpy().item()\n",
    "                if sent_porb < 0.7:\n",
    "                    break     \n",
    "\n",
    "            del_list = del_idx.squeeze(0).cpu().tolist() # list\n",
    "            del_sen =''\n",
    "            for x in range(len(del_list)):            \n",
    "                token = num2token[del_list[x]].strip('Ġ')\n",
    "                del_sen += token\n",
    "                del_sen += ' '\n",
    "            del_sen = del_sen.strip()\n",
    "\n",
    "            del_percent = 100-(del_idx.shape[1])/(token_idx.shape[1]) * 100\n",
    "\n",
    "            enc_out = genmodel.encoder(del_idx)\n",
    "#             dec_out, vocab_out = genmodel.decoder(enc_out, token_idx, attribute)\n",
    "\n",
    "#             dec_tokens, dec_sen = genmodel.dec2sen(vocab_out)\n",
    "\n",
    "#             gen_sen_1 = genmodel.generated_sentence(enc_out, attribute, ori_length)\n",
    "            gen_sen_2 = genmodel.generated_sentence(enc_out, fake_attribute, ori_length)\n",
    "\n",
    "            print('Original Attribute: ', sentiment)\n",
    "            print('Original Sentence: ', sentence)\n",
    "            print('Delete Sentence: {}, {}%'.format(del_sen, del_percent))\n",
    "#             print('Reconstruction(decoder) Sentence: ', dec_sen)\n",
    "#             print('Reconstruction(generator) Sentence', sentiment, ': ', gen_sen_1.rstrip('<|endoftext|>'))      \n",
    "#             print('Style transfer(generator) Sentence', abs(1-sentiment), ': ', gen_sen_2.rstrip('<|endoftext|>'))\n",
    "            print('Style transfer(generator) Sentence', fake_attribute.cpu().numpy().tolist()[0], ': ', gen_sen_2.rstrip('<|endoftext|>'))    \n",
    "            print('')        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [09:07<09:07, 547.45s/it]\u001b[A\n",
      "100%|██████████| 2/2 [18:58<00:00, 569.13s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "## test data 저장\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/DATA/joosung/controllable_english/classifier/\")\n",
    "from dis_model import *\n",
    "dismodel = findattribute().cuda()\n",
    "dismodel_name='cls_model_3'\n",
    "dismodel.load_state_dict(torch.load('../classifier/models/{}'.format(dismodel_name)))\n",
    "dismodel.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gen_model import *\n",
    "genmodel = styletransfer().cuda()\n",
    "\n",
    "data_path = \"/DATA/joosung/sentiment_data/Sentiment-and-Style-Transfer-master/data\"\n",
    "yelp_neg_path = data_path + \"/yelp/sentiment.test.0\"\n",
    "yelp_neg_open = open(yelp_neg_path, \"r\")\n",
    "yelp_neg_dataset = yelp_neg_open.readlines()\n",
    "neg_len = len(yelp_neg_dataset)\n",
    "yelp_neg_open.close()\n",
    "\n",
    "yelp_pos_path = data_path + \"/yelp/sentiment.test.1\"\n",
    "yelp_pos_open = open(yelp_pos_path, \"r\")\n",
    "yelp_pos_dataset = yelp_pos_open.readlines()\n",
    "pos_len = len(yelp_pos_dataset)\n",
    "yelp_pos_open.close()\n",
    "\n",
    "stop_point = pos_len\n",
    "\n",
    "PAD_IDX = 50258\n",
    "\n",
    "name_list = [1,2]\n",
    "prob = 0.6\n",
    "save_prob = '06'\n",
    "for name in tqdm(range(len(name_list))):\n",
    "    for m in range(4):\n",
    "        if m==0:\n",
    "            per = 0\n",
    "        elif m==1:\n",
    "            per = 50\n",
    "        elif m==2:\n",
    "            per = 60\n",
    "        else:\n",
    "            per = 70\n",
    "            \n",
    "        genmodel_name='gen_model_' + str(name_list[name])\n",
    "        genmodel.load_state_dict(torch.load('./models/{}'.format(genmodel_name)))\n",
    "        genmodel.eval()\n",
    "        model0 = 'sentiment.test.0.' + 'joo' + str(name_list[name])+'_'+str(per)+'_'+str(save_prob)\n",
    "        model1 = 'sentiment.test.1.' + 'joo' + str(name_list[name])+'_'+str(per)+'_'+str(save_prob)\n",
    "        f0 = open(model0, 'w')\n",
    "        f1 = open(model1, 'w')\n",
    "\n",
    "        for start in range(stop_point):\n",
    "            \"\"\"data start point\"\"\"\n",
    "            neg_start = start\n",
    "            pos_start = start\n",
    "\n",
    "            \"\"\"data setting\"\"\"\n",
    "            neg_sentence = yelp_neg_dataset[neg_start].strip()\n",
    "            pos_sentence = yelp_pos_dataset[pos_start].strip()                \n",
    "\n",
    "            neg_labels = [] # negative labels\n",
    "            neg_labels.append([1,0])\n",
    "            neg_attribute = torch.from_numpy(np.asarray(neg_labels)).type(torch.FloatTensor).cuda()\n",
    "\n",
    "            pos_labels = [] # positive labels\n",
    "            pos_labels.append([0,1])\n",
    "            pos_attribute = torch.from_numpy(np.asarray(pos_labels)).type(torch.FloatTensor).cuda()\n",
    "\n",
    "            sentences = [neg_sentence, pos_sentence]\n",
    "            attributes = [neg_attribute, pos_attribute]\n",
    "            fake_attributes = [pos_attribute, neg_attribute]\n",
    "            sentiments = [0, 1]\n",
    "            \"\"\"data input\"\"\"\n",
    "            for i in range(2):\n",
    "                # k=0: negative, k=1: positive\n",
    "                sentence = sentences[i]\n",
    "                attribute = attributes[i] # for decoder\n",
    "                fake_attribute = attributes[abs(1-i)] # for generate\n",
    "                sentiment = sentiments[i] # for delete\n",
    "\n",
    "                token_idx = torch.tensor(gpt_tokenizer.encode(sentence)).unsqueeze(0).cuda()\n",
    "                ori_length = token_idx.shape[1]\n",
    "\n",
    "                # delete model\n",
    "                if per == 0:\n",
    "                    max_len = int(token_idx.shape[1]-1) # 0%\n",
    "                elif per == 50:\n",
    "                    max_len = int(token_idx.shape[1]/2) # 50%\n",
    "                elif per == 60:\n",
    "                    max_len = int(token_idx.shape[1]/10*4) # 60%\n",
    "                else:\n",
    "                    max_len = int(token_idx.shape[1]/10*3) # 70%            \n",
    "    #             max_len = 0 # 100%\n",
    "\n",
    "#                 dis_out = dismodel.discriminator(token_idx)    \n",
    "#                 sentiment = dis_out.argmax(1).cpu().item() ## for delete\n",
    "\n",
    "                del_idx = token_idx\n",
    "                for k in range(max_len):\n",
    "                    del_idx = dismodel.att_prob(del_idx, sentiment)  \n",
    "                    dis_out = dismodel.discriminator(del_idx)    \n",
    "                    sent_porb = F.softmax(dis_out, 1).squeeze(0)[sentiment].cpu().detach().numpy().item()\n",
    "                    if sent_porb < prob: # 0.7\n",
    "                        break \n",
    "\n",
    "                del_list = del_idx.squeeze(0).cpu().tolist() # list\n",
    "                del_sen =''\n",
    "                for x in range(len(del_list)):            \n",
    "                    token = num2token[del_list[x]].strip('Ġ')\n",
    "                    del_sen += token\n",
    "                    del_sen += ' '\n",
    "                del_sen = del_sen.strip()\n",
    "\n",
    "                del_percent = 100-(del_idx.shape[1])/(token_idx.shape[1]) * 100\n",
    "\n",
    "                enc_out = genmodel.encoder(del_idx)\n",
    "                dec_out, vocab_out = genmodel.decoder(enc_out, token_idx, attribute)\n",
    "\n",
    "                dec_tokens, dec_sen = genmodel.dec2sen(vocab_out)\n",
    "\n",
    "                gen_sen_2 = genmodel.generated_sentence(enc_out, fake_attribute, ori_length).replace('<|endoftext|>', '')\n",
    "\n",
    "                if i == 0:\n",
    "                    f0.write(sentence+'\\t'+gen_sen_2+'\\t'+str(sentiment)+'\\n')\n",
    "                if i == 1:\n",
    "                    f1.write(sentence+'\\t'+gen_sen_2+'\\t'+str(sentiment)+'\\n')\n",
    "        f0.close()\n",
    "        f1.close()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specific model test data\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/DATA/joosung/controllable_english/classifier/\")\n",
    "from dis_model import *\n",
    "dismodel = findattribute().cuda()\n",
    "dismodel_name='cls_model_3'\n",
    "dismodel.load_state_dict(torch.load('../classifier/models/{}'.format(dismodel_name)))\n",
    "dismodel.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gen_model import *\n",
    "genmodel = styletransfer().cuda()\n",
    "\n",
    "data_path = \"/DATA/joosung/sentiment_data/Sentiment-and-Style-Transfer-master/data\"\n",
    "yelp_neg_path = data_path + \"/yelp/sentiment.test.0\"\n",
    "yelp_neg_open = open(yelp_neg_path, \"r\")\n",
    "yelp_neg_dataset = yelp_neg_open.readlines()\n",
    "neg_len = len(yelp_neg_dataset)\n",
    "yelp_neg_open.close()\n",
    "\n",
    "yelp_pos_path = data_path + \"/yelp/sentiment.test.1\"\n",
    "yelp_pos_open = open(yelp_pos_path, \"r\")\n",
    "yelp_pos_dataset = yelp_pos_open.readlines()\n",
    "pos_len = len(yelp_pos_dataset)\n",
    "yelp_pos_open.close()\n",
    "\n",
    "stop_point = pos_len\n",
    "\n",
    "PAD_IDX = 50258\n",
    "\n",
    "name_list = [1,2,3,4,5,6]\n",
    "for name in tqdm(range(len(name_list))):\n",
    "    genmodel_name='gen_model_' + str(name_list[name])\n",
    "    genmodel.load_state_dict(torch.load('./models/{}'.format(genmodel_name)))\n",
    "    genmodel.eval()\n",
    "    model0 = 'sentiment.test.0.' + 'joo' + str(name_list[name])\n",
    "    model1 = 'sentiment.test.1.' + 'joo' + str(name_list[name])\n",
    "    f0 = open(model0, 'w')\n",
    "    f1 = open(model1, 'w')\n",
    "\n",
    "    for start in range(stop_point):\n",
    "        \"\"\"data start point\"\"\"\n",
    "        neg_start = start\n",
    "        pos_start = start\n",
    "\n",
    "        \"\"\"data setting\"\"\"\n",
    "        neg_sentence = yelp_neg_dataset[neg_start].strip()\n",
    "        pos_sentence = yelp_pos_dataset[pos_start].strip()                \n",
    "\n",
    "        neg_labels = [] # negative labels\n",
    "        neg_labels.append([1,0])\n",
    "        neg_attribute = torch.from_numpy(np.asarray(neg_labels)).type(torch.FloatTensor).cuda()\n",
    "\n",
    "        pos_labels = [] # positive labels\n",
    "        pos_labels.append([0,1])\n",
    "        pos_attribute = torch.from_numpy(np.asarray(pos_labels)).type(torch.FloatTensor).cuda()\n",
    "\n",
    "        sentences = [neg_sentence, pos_sentence]\n",
    "        attributes = [neg_attribute, pos_attribute]\n",
    "        fake_attributes = [pos_attribute, neg_attribute]\n",
    "        sentiments = [0, 1]\n",
    "        \"\"\"data input\"\"\"\n",
    "        for i in range(2):\n",
    "            # k=0: negative, k=1: positive\n",
    "            sentence = sentences[i]\n",
    "            attribute = attributes[i] # for decoder\n",
    "            fake_attribute = attributes[abs(1-i)] # for generate\n",
    "            sentiment = sentiments[i] # for delete\n",
    "\n",
    "            token_idx = torch.tensor(gpt_tokenizer.encode(sentence)).unsqueeze(0).cuda()\n",
    "            ori_length = token_idx.shape[1]\n",
    "\n",
    "            # delete model\n",
    "            max_len = int(token_idx.shape[1]/10*4) # 60%\n",
    "            \n",
    "\n",
    "            dis_out = dismodel.discriminator(token_idx)    \n",
    "#             sentiment = dis_out.argmax(1).cpu().item() ## 변경점 for delete\n",
    "\n",
    "            del_idx = token_idx\n",
    "            for k in range(max_len):\n",
    "                del_idx = dismodel.att_prob(del_idx, sentiment)  \n",
    "                dis_out = dismodel.discriminator(del_idx)    \n",
    "                sent_porb = F.softmax(dis_out, 1).squeeze(0)[sentiment].cpu().detach().numpy().item()\n",
    "                if sent_porb < 0.6: # 0.7\n",
    "                    break \n",
    "\n",
    "            del_list = del_idx.squeeze(0).cpu().tolist() # list\n",
    "            del_sen =''\n",
    "            for x in range(len(del_list)):            \n",
    "                token = num2token[del_list[x]].strip('Ġ')\n",
    "                del_sen += token\n",
    "                del_sen += ' '\n",
    "            del_sen = del_sen.strip()\n",
    "\n",
    "            del_percent = 100-(del_idx.shape[1])/(token_idx.shape[1]) * 100\n",
    "\n",
    "            enc_out = genmodel.encoder(del_idx)\n",
    "            dec_out, vocab_out = genmodel.decoder(enc_out, token_idx, attribute)\n",
    "\n",
    "            dec_tokens, dec_sen = genmodel.dec2sen(vocab_out)\n",
    "\n",
    "            gen_sen_2 = genmodel.generated_sentence(enc_out, fake_attribute, ori_length).replace('<|endoftext|>', '')\n",
    "\n",
    "            if i == 0:\n",
    "                f0.write(sentence+'\\t'+gen_sen_2+'\\t'+str(sentiment)+'\\n')\n",
    "            if i == 1:\n",
    "                f1.write(sentence+'\\t'+gen_sen_2+'\\t'+str(sentiment)+'\\n')\n",
    "    f0.close()\n",
    "    f1.close()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
